{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQuJ8Zz5gCrfdAbbwnGlaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucok-man/analisis-sentimen-review-aplikasi-SIREKAP-KPU/blob/main/Analisis_Sentimen_Review_Aplikasi_SIREKAP_KPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2: Install Library yang Dibutuhkan\n",
        "\n",
        "**Penjelasan:**\n",
        "Install library yang diperlukan untuk scraping data dari Play Store, pemrosesan teks Bahasa Indonesia, dan analisis sentimen. Library `google-play-scraper` untuk mengambil review, `Sastrawi` untuk stemming Bahasa Indonesia, dan `nltk` untuk pemrosesan teks."
      ],
      "metadata": {
        "id": "pzIYTHrcUReM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-play-scraper\n",
        "!pip install pymongo\n",
        "!pip install Sastrawi\n",
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "!pip install pandarallel\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "kPJq9Uf_UcUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3: Import Library\n",
        "\n",
        "**Penjelasan:**\n",
        "Import semua library yang akan digunakan dalam project ini. Library-library ini mencakup tools untuk scraping, manipulasi data, pemrosesan teks, machine learning, dan visualisasi."
      ],
      "metadata": {
        "id": "FnaUmxExU7wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import app, Sort, reviews_all\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download resource NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "kTA-WnKyVCHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 4: Scraping Review dari Play Store\n",
        "\n",
        "**Penjelasan:**\n",
        "Lakukan scraping untuk mengumpulkan semua review aplikasi SIREKAP KPU dari Google Play Store. ID aplikasi adalah `id.go.kpu.sirekap2024`. Parameter `lang='id'` dan `country='id'` memastikan kita mendapatkan review dari pengguna Indonesia dalam Bahasa Indonesia."
      ],
      "metadata": {
        "id": "GLUgRjJuVXvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping semua review\n",
        "app_id = \"id.go.kpu.sirekap2024\"\n",
        "result = reviews_all(\n",
        "    app_id,\n",
        "    sleep_milliseconds=0,\n",
        "    lang='id',              # Bahasa Indonesia\n",
        "    country='id',           # Negara Indonesia\n",
        "    sort=Sort.NEWEST        # Urut berdasarkan terbaru\n",
        ")\n",
        "\n",
        "print(f\"Berhasil mengumpulkan {len(result)} review\")"
      ],
      "metadata": {
        "id": "t_sB7vy5VqS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 5: Konversi ke DataFrame\n",
        "\n",
        "**Penjelasan:**\n",
        "Ubah hasil scraping yang berupa list of dictionaries menjadi pandas DataFrame agar lebih mudah dimanipulasi dan dianalisis. DataFrame menyediakan struktur tabular yang memudahkan operasi data."
      ],
      "metadata": {
        "id": "w9OY70w-WWBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ubah hasil scraping ke DataFrame\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "# Tampilkan 5 data pertama\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wPDEiim9Wg_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 6: Filter dan Urutkan Data\n",
        "\n",
        "**Penjelasan:**\n",
        "Dari semua kolom yang tersedia, kita hanya memerlukan kolom `userName` (nama pengguna), `score` (rating 1-5), `at` (tanggal review), dan `content` (isi review). Data kemudian diurutkan berdasarkan tanggal terbaru agar review terkini muncul di atas."
      ],
      "metadata": {
        "id": "rvV1AfSNZobD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil kolom penting untuk analisis\n",
        "df_clean = df[['userName', 'score', 'at', 'content']].copy()\n",
        "\n",
        "# Urutkan berdasarkan tanggal terbaru\n",
        "df_clean = df_clean.sort_values(by='at', ascending=False)\n",
        "\n",
        "# Reset index\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total review: {len(df_clean)}\")\n",
        "df_clean.head()\n"
      ],
      "metadata": {
        "id": "xoA2DM3SZu-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 7: Buat Label Sentimen\n",
        "\n",
        "**Penjelasan:**\n",
        "Buat label sentimen berdasarkan rating yang diberikan pengguna. Rating 1-2 dikategorikan sebagai **Negatif**, rating 3 sebagai **Netral**, dan rating 4-5 sebagai **Positif**. Fungsi `pelabelan()` akan otomatis mengklasifikasikan setiap review."
      ],
      "metadata": {
        "id": "XpIbdhJZZ4KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pelabelan(score):\n",
        "    if score <= 2:\n",
        "        return \"Negatif\"\n",
        "    elif score == 3:\n",
        "        return \"Netral\"\n",
        "    else:\n",
        "        return \"Positif\"\n",
        "\n",
        "# Terapkan pelabelan\n",
        "df_clean['Label'] = df_clean['score'].apply(pelabelan)\n",
        "\n",
        "# Tampilkan distribusi label\n",
        "print(df_clean['Label'].value_counts())\n",
        "df_clean.head(10)"
      ],
      "metadata": {
        "id": "Avl7LY91Z5f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 8: Hapus Data Netral\n",
        "\n",
        "**Penjelasan:**\n",
        "Untuk analisis sentimen binary (positif vs negatif), kita hapus review dengan sentimen netral karena bisa menimbulkan ambiguitas. Fokus analisis adalah pada review yang jelas positif atau negatif saja."
      ],
      "metadata": {
        "id": "nH6CCbFYaxpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus review dengan sentimen netral\n",
        "df_clean = df_clean[df_clean['Label'] != 'Netral'].copy()\n",
        "\n",
        "# Reset index\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total review setelah menghapus netral: {len(df_clean)}\")\n",
        "print(f\"\\nDistribusi label akhir:\\n{df_clean['Label'].value_counts()}\")"
      ],
      "metadata": {
        "id": "x9fpUP4qbjg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 9: Simpan Data Mentah\n",
        "\n",
        "**Penjelasan:**\n",
        "Simpan data yang sudah diberi label ke file CSV sebagai backup. File ini berguna jika nanti kita perlu mengulang analisis tanpa harus scraping ulang dari Play Store"
      ],
      "metadata": {
        "id": "8GipVbdCdAcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan untuk backup\n",
        "df_clean.to_csv(\"data_review_labeled.csv\", index=False)\n",
        "print(\"Data berhasil disimpan ke 'data_review_labeled.csv'\")"
      ],
      "metadata": {
        "id": "_K8ixCdodCr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 10: Case Folding & Text Cleaning\n",
        "\n",
        "**Penjelasan:**\n",
        "Bersihkan teks dari elemen yang tidak diperlukan. Proses ini meliputi: mengubah semua huruf menjadi lowercase, menghapus URL, mention, hashtag, angka, dan karakter khusus. Tujuannya agar teks menjadi konsisten dan mudah diproses oleh model."
      ],
      "metadata": {
        "id": "upr_UvKzdNu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Ubah ke huruf kecil\n",
        "    text = text.lower()\n",
        "\n",
        "    # Hapus URL\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Hapus mention dan hashtag\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Hapus angka\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Hapus karakter khusus, hanya simpan huruf dan spasi\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Hapus spasi berlebih\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Terapkan cleaning\n",
        "df_clean['text_clean'] = df_clean['content'].apply(clean_text)\n",
        "\n",
        "# Tampilkan hasil\n",
        "df_clean[['content', 'text_clean']].head()\n"
      ],
      "metadata": {
        "id": "maECz7GgdQNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 11: Stopword Removal\n",
        "\n",
        "**Penjelasan:**\n",
        "Hapus kata-kata umum (stopwords) yang tidak membawa makna penting dalam analisis sentimen, seperti \"yang\", \"dan\", \"di\", \"dari\", dll. NLTK menyediakan daftar stopwords untuk Bahasa Indonesia yang sudah cukup lengkap."
      ],
      "metadata": {
        "id": "ZzZ7ebIijndi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopwords Bahasa Indonesia\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Terapkan stopword removal\n",
        "df_clean['text_stopword'] = df_clean['text_clean'].apply(remove_stopwords)\n",
        "\n",
        "# Tampilkan hasil\n",
        "df_clean[['text_clean', 'text_stopword']].head()"
      ],
      "metadata": {
        "id": "MC8jeWJ5john"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 12: Tokenizing\n",
        "\n",
        "**Penjelasan:**\n",
        "Tokenizing adalah proses memecah teks menjadi unit-unit kecil yang disebut token (biasanya kata). Proses ini penting untuk tahap stemming dan analisis lebih lanjut. NLTK menyediakan fungsi `word_tokenize()` untuk Bahasa Indonesia."
      ],
      "metadata": {
        "id": "OetYdBnvoW9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "df_clean['text_tokens'] = df_clean['text_stopword'].apply(word_tokenize)\n",
        "\n",
        "# Tampilkan hasil\n",
        "df_clean[['text_stopword', 'text_tokens']].head()\n"
      ],
      "metadata": {
        "id": "ISUTAXDDH5m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 13: Stemming dengan Sastrawi\n",
        "\n",
        "**Penjelasan:**\n",
        "Stemming mengubah kata-kata ke bentuk dasarnya, misalnya \"berjalan\" menjadi \"jalan\", \"berlari\" menjadi \"lari\". Library Sastrawi menggunakan algoritma Nazief dan Adriani yang dirancang khusus untuk Bahasa Indonesia."
      ],
      "metadata": {
        "id": "mjuyA97oaM9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandarallel import pandarallel\n",
        "from functools import lru_cache\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Inisialisasi pandarallel (aktifkan progress bar)\n",
        "pandarallel.initialize(progress_bar=True)\n",
        "\n",
        "# Buat stemmer\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "# Cache hasil stemming supaya gak dihitung ulang\n",
        "@lru_cache(maxsize=None)\n",
        "def cached_stem(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "def stemming_text(tokens):\n",
        "    return [cached_stem(token) for token in tokens]\n",
        "\n",
        "# Jalankan stemming secara paralel\n",
        "df_clean[\"text_stemmed_list\"] = df_clean[\"text_tokens\"].parallel_apply(stemming_text)\n",
        "\n",
        "# # Gabungkan token menjadi string\n",
        "df_clean['text_final'] = df_clean['text_stemmed_list'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# tampilkan hasil\n",
        "df_clean[['text_stopword', 'text_final']].head()\n"
      ],
      "metadata": {
        "id": "CHyxjZSYaPbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 14: Hapus Teks Kosong\n",
        "\n",
        "**Penjelasan:**\n",
        "Setelah preprocessing, beberapa review mungkin menjadi kosong (hanya berisi stopwords atau karakter khusus). Review kosong ini perlu dihapus karena tidak memiliki informasi untuk analisis sentimen."
      ],
      "metadata": {
        "id": "RFKL0jLHdC9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus baris dengan teks kosong setelah preprocessing\n",
        "df_clean = df_clean[df_clean['text_final'].str.strip() != ''].copy()\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total review setelah cleaning: {len(df_clean)}\")"
      ],
      "metadata": {
        "id": "NJqw7CRVdGaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 15: Simpan Hasil Preprocessing\n",
        "\n",
        "**Penjelasan:**\n",
        "Simpan hasil preprocessing ke file CSV baru. File ini berisi teks yang sudah bersih dan siap untuk pemodelan machine learning. Menyimpan hasil preprocessing menghemat waktu jika perlu training model ulang."
      ],
      "metadata": {
        "id": "AaZiy_9sdKbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil preprocessing\n",
        "df_clean.to_csv('data_preprocessed.csv', index=False)\n",
        "print(\"Data preprocessing berhasil disimpan ke 'data_preprocessed.csv'!\")"
      ],
      "metadata": {
        "id": "HBGVpotAdOLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 16: Split Data Training & Testing\n",
        "\n",
        "**Penjelasan:**\n",
        "Bagi dataset menjadi data training (80%) dan data testing (20%). Data training digunakan untuk melatih model, sedangkan data testing untuk mengevaluasi performa model pada data yang belum pernah dilihat. Parameter `stratify=y` memastikan proporsi label Positif dan Negatif seimbang di kedua set."
      ],
      "metadata": {
        "id": "36OSW1cwdQ9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Pisahkan fitur (X) dan target (y)\n",
        "X = df_clean['text_final']\n",
        "y = df_clean['Label']\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      # 20% untuk testing\n",
        "    random_state=42,\n",
        "    stratify=y          # Memastikan proporsi label seimbang\n",
        ")\n",
        "\n",
        "print(f\"Jumlah data training: {len(X_train)}\")\n",
        "print(f\"Jumlah data testing: {len(X_test)}\")\n",
        "print(f\"\\nDistribusi label training:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nDistribusi label testing:\\n{y_test.value_counts()}\")\n"
      ],
      "metadata": {
        "id": "aCPganKMdUZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 17: TF-IDF Vectorization\n",
        "\n",
        "**Penjelasan:**\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) mengubah teks menjadi representasi numerik yang dapat diproses oleh algoritma machine learning. TF-IDF memberikan bobot lebih tinggi pada kata yang penting dan unik dalam dokumen. Parameter `max_features=5000` membatasi jumlah fitur untuk efisiensi."
      ],
      "metadata": {
        "id": "NmIUYcHPdcX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit dan transform data training\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "\n",
        "# Transform data testing (hanya transform, tidak fit)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(f\"Shape X_train_tfidf: {X_train_tfidf.shape}\")\n",
        "print(f\"Shape X_test_tfidf: {X_test_tfidf.shape}\")\n",
        "print(f\"Jumlah fitur (kata unik): {len(tfidf.get_feature_names_out())}\")\n"
      ],
      "metadata": {
        "id": "ZLd8OfEtddfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 18: Training Model Naive Bayes\n",
        "\n",
        "**Penjelasan:**\n",
        "Multinomial Naive Bayes adalah algoritma yang cocok untuk klasifikasi teks karena bekerja baik dengan data count/frequency. Model ini menggunakan teorema Bayes dengan asumsi independensi antar fitur. Proses training akan mempelajari pola dari data training untuk memprediksi sentimen."
      ],
      "metadata": {
        "id": "AUD85C98diKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Inisialisasi dan training model\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi pada data testing\n",
        "y_pred = model_nb.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Model berhasil dilatih!\")\n",
        "print(f\"Jumlah prediksi: {len(y_pred)}\")"
      ],
      "metadata": {
        "id": "HB1QKyamdky8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 19: Evaluasi Model dengan Metrik\n",
        "\n",
        "**Penjelasan:**\n",
        "Evaluasi performa model menggunakan berbagai metrik:\n",
        "\n",
        "- **Accuracy**: Persentase prediksi yang benar\n",
        "- **Precision**: Dari yang diprediksi positif, berapa yang benar-benar positif\n",
        "- **Recall**: Dari yang sebenarnya positif, berapa yang berhasil diprediksi\n",
        "- **F1-Score**: Harmonic mean dari precision dan recall"
      ],
      "metadata": {
        "id": "710_ML0qdqFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Hitung metrik\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Negatif', average='binary')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Negatif', average='binary')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Negatif', average='binary')\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"HASIL EVALUASI MODEL\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Accuracy  : {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision : {precision:.4f}\")\n",
        "print(f\"Recall    : {recall:.4f}\")\n",
        "print(f\"F1-Score  : {f1:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Classification report lengkap\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))"
      ],
      "metadata": {
        "id": "rEJ2aEXrduAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 20: Visualisasi Confusion Matrix\n",
        "\n",
        "**Penjelasan:**\n",
        "Confusion Matrix menampilkan perbandingan antara label aktual dan label yang diprediksi model. Matrix ini membantu memahami jenis kesalahan yang dibuat model (False Positive vs False Negative) dan memberikan insight untuk improvement."
      ],
      "metadata": {
        "id": "WsuNGUhnd9yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Hitung confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['Negatif', 'Positif'])\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negatif', 'Positif'],\n",
        "            yticklabels=['Negatif', 'Positif'],\n",
        "            cbar_kws={'label': 'Jumlah'})\n",
        "plt.xlabel('Prediksi', fontsize=12)\n",
        "plt.ylabel('Aktual', fontsize=12)\n",
        "plt.title('Confusion Matrix - Analisis Sentimen SIREKAP KPU', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretasi hasil\n",
        "print(f\"\\nTrue Negative  : {cm[0][0]} (Review negatif diprediksi negatif ✓)\")\n",
        "print(f\"False Positive : {cm[0][1]} (Review negatif diprediksi positif ✗)\")\n",
        "print(f\"False Negative : {cm[1][0]} (Review positif diprediksi negatif ✗)\")\n",
        "print(f\"True Positive  : {cm[1][1]} (Review positif diprediksi positif ✓)\")\n"
      ],
      "metadata": {
        "id": "elr975iIeARU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 21: Fungsi Prediksi Review Baru\n",
        "\n",
        "**Penjelasan:**\n",
        "Buat fungsi untuk memprediksi sentimen dari review baru. Fungsi ini akan melakukan semua tahap preprocessing (cleaning, stopword removal, stemming) dan menggunakan model yang sudah dilatih untuk prediksi. Output berupa sentimen dan confidence score."
      ],
      "metadata": {
        "id": "d_ABrDaLeEZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    # Preprocessing\n",
        "    cleaned = clean_text(text)\n",
        "    no_stopwords = remove_stopwords(cleaned)\n",
        "    tokens = word_tokenize(no_stopwords)\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    final_text = ' '.join(stemmed)\n",
        "\n",
        "    # Vectorization\n",
        "    text_tfidf = tfidf.transform([final_text])\n",
        "\n",
        "    # Prediksi\n",
        "    prediction = model_nb.predict(text_tfidf)[0]\n",
        "    probability = model_nb.predict_proba(text_tfidf)[0]\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'sentiment': prediction,\n",
        "        'confidence': max(probability) * 100\n",
        "    }\n",
        "\n",
        "# Contoh penggunaan\n",
        "contoh_reviews = [\n",
        "    \"Aplikasi sangat bagus dan membantu sekali!\",\n",
        "    \"Jelek banget, sering error dan lambat\",\n",
        "    \"Aplikasi lumayan, tapi masih banyak bug yang harus diperbaiki\"\n",
        "]\n",
        "\n",
        "print(\"HASIL PREDIKSI:\")\n",
        "print(\"=\" * 70)\n",
        "for review in contoh_reviews:\n",
        "    result = predict_sentiment(review)\n",
        "    print(f\"Review: {result['text']}\")\n",
        "    print(f\"Sentimen: {result['sentiment']} (Confidence: {result['confidence']:.2f}%)\")\n",
        "    print(\"-\" * 70)\n"
      ],
      "metadata": {
        "id": "6dvgwLsOeI8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 22: Visualisasi Distribusi Sentimen\n",
        "\n",
        "**Penjelasan:**\n",
        "Buat visualisasi untuk melihat distribusi sentimen dalam dataset. Grafik bar chart ini menunjukkan jumlah dan persentase review positif vs negatif, memberikan gambaran umum tentang persepsi pengguna terhadap aplikasi SIREKAP KPU."
      ],
      "metadata": {
        "id": "SPZLL_dcefmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung distribusi sentimen\n",
        "sentiment_dist = df_clean['Label'].value_counts()\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#ff6b6b', '#51cf66']\n",
        "bars = sentiment_dist.plot(kind='bar', color=colors, alpha=0.8)\n",
        "plt.title('Distribusi Sentimen Review SIREKAP KPU', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Sentimen', fontsize=12)\n",
        "plt.ylabel('Jumlah Review', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Tambahkan nilai di atas bar\n",
        "for i, v in enumerate(sentiment_dist.values):\n",
        "    plt.text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Persentase\n",
        "print(\"\\nPersentase Sentimen:\")\n",
        "for label, count in sentiment_dist.items():\n",
        "    percentage = (count / len(df_clean)) * 100\n",
        "    print(f\"{label}: {count} review ({percentage:.2f}%)\")\n"
      ],
      "metadata": {
        "id": "Vx5AA055egy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 23: Word Cloud Sentimen Positif & Negatif\n",
        "\n",
        "**Penjelasan:**\n",
        "Word Cloud memvisualisasikan kata-kata yang paling sering muncul dalam review. Ukuran kata menunjukkan frekuensi kemunculannya. Dengan membuat word cloud terpisah untuk sentimen positif dan negatif, kita dapat melihat kata-kata kunci yang menjadi ciri khas masing-masing sentimen."
      ],
      "metadata": {
        "id": "_Hs4a-oqelFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Word Cloud untuk Sentimen Positif\n",
        "text_positif = ' '.join(df_clean[df_clean['Label'] == 'Positif']['text_final'])\n",
        "\n",
        "wordcloud_pos = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='Greens'\n",
        ").generate(text_positif)\n",
        "\n",
        "# Word Cloud untuk Sentimen Negatif\n",
        "text_negatif = ' '.join(df_clean[df_clean['Label'] == 'Negatif']['text_final'])\n",
        "\n",
        "wordcloud_neg = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='Reds'\n",
        ").generate(text_negatif)\n",
        "\n",
        "#  Visualisasi Kedua Word Cloud\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Word cloud positif\n",
        "axes[0].imshow(wordcloud_pos, interpolation='bilinear')\n",
        "axes[0].set_title(\n",
        "    'Word Cloud - Sentimen Positif',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    color='green'\n",
        ")\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Word cloud negatif\n",
        "axes[1].imshow(wordcloud_neg, interpolation='bilinear')\n",
        "axes[1].set_title(\n",
        "    'Word Cloud - Sentimen Negatif',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    color='red'\n",
        ")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7KqXkLKlepeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 24: Simpan Model & Vectorizer\n",
        "\n",
        "**Penjelasan:**\n",
        "Simpan model yang sudah dilatih dan TF-IDF vectorizer ke file menggunakan pickle. File-file ini dapat digunakan kembali untuk prediksi di masa depan tanpa perlu training ulang. Ini sangat berguna untuk deployment aplikasi atau analisis berkelanjutan."
      ],
      "metadata": {
        "id": "mDNI3n1RfG7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Simpan model\n",
        "with open('model_sentiment.pkl', 'wb') as f:\n",
        "    pickle.dump(model_nb, f)\n",
        "\n",
        "# Simpan vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf, f)\n",
        "\n",
        "print(\"✓ Model berhasil disimpan ke 'model_sentiment.pkl'\")\n",
        "print(\"✓ Vectorizer berhasil disimpan ke 'tfidf_vectorizer.pkl'\")\n"
      ],
      "metadata": {
        "id": "77-ICBEwfJhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 25: Load Model untuk Penggunaan Kembali\n",
        "\n",
        "**Penjelasan:**\n",
        "Demonstrasi cara memuat kembali model dan vectorizer yang sudah disimpan. Ini berguna ketika ingin menggunakan model di session baru tanpa perlu training ulang. Setelah dimuat, model dapat langsung digunakan untuk prediksi."
      ],
      "metadata": {
        "id": "I4HUC4UAfMbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model dan vectorizer\n",
        "with open('model_sentiment.pkl', 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    loaded_tfidf = pickle.load(f)\n",
        "\n",
        "print(\"✓ Model dan vectorizer berhasil dimuat!\")\n",
        "\n",
        "# Test prediksi dengan model yang dimuat\n",
        "test_text = \"Aplikasi ini sangat membantu dan mudah digunakan\"\n",
        "test_clean = clean_text(test_text)\n",
        "test_nostop = remove_stopwords(test_clean)\n",
        "test_tokens = word_tokenize(test_nostop)\n",
        "test_stemmed = ' '.join([stemmer.stem(word) for word in test_tokens])\n",
        "test_tfidf = loaded_tfidf.transform([test_stemmed])\n",
        "prediction = loaded_model.predict(test_tfidf)[0]\n",
        "\n",
        "print(f\"\\nTest Prediksi:\")\n",
        "print(f\"Text: {test_text}\")\n",
        "print(f\"Prediksi: {prediction}\")"
      ],
      "metadata": {
        "id": "JR9AJEm6fSCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}